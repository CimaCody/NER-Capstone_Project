{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertForTokenClassification, BertTokenizerFast\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "metadata": {
        "id": "wbyydwxNOxld"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kTLNXQHQWNH",
        "outputId": "8ce4b5dd-0cb6-4cda-e6a6-1226e5b363dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AIDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, label_map, max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label_map = label_map\n",
        "        self.max_length = max_length\n",
        "        self.texts = []\n",
        "        self.labels = []\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            tokens = []\n",
        "            tag_labels = []\n",
        "            for line in file:\n",
        "                line = line.strip()\n",
        "                if line == \"\" or line.startswith(\"-DOCSTART-\"):\n",
        "                    if tokens:\n",
        "                        self.texts.append(tokens)\n",
        "                        self.labels.append(tag_labels)\n",
        "                        tokens, tag_labels = [], []\n",
        "                    continue\n",
        "                parts = line.split()\n",
        "                tokens.append(parts[0])\n",
        "                tag_labels.append(parts[-1])\n",
        "\n",
        "            if tokens:\n",
        "                self.texts.append(tokens)\n",
        "                self.labels.append(tag_labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.texts[idx]\n",
        "        labels = self.labels[idx]\n",
        "\n",
        "        tokenized_inputs = self.tokenizer(\n",
        "            tokens,\n",
        "            is_split_into_words=True,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = tokenized_inputs['input_ids'].squeeze(0)\n",
        "        attention_mask = tokenized_inputs['attention_mask'].squeeze(0)\n",
        "\n",
        "        # Convert labels to indices using the label map\n",
        "        label_ids = [self.label_map.get(label, self.label_map['O']) for label in labels]\n",
        "\n",
        "        # Handle subword tokens\n",
        "        new_labels = []\n",
        "        previous_word_idx = None\n",
        "        for word_idx in tokenized_inputs.word_ids(batch_index=0):\n",
        "            if word_idx is None or word_idx == previous_word_idx:\n",
        "                new_labels.append(-100)\n",
        "            else:\n",
        "                new_labels.append(label_ids[word_idx])\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        label_ids = torch.tensor(new_labels[:self.max_length] + [-100] * (self.max_length - len(new_labels)), dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': label_ids\n",
        "        }\n"
      ],
      "metadata": {
        "id": "osAjpy7CWMdT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label_map = {\n",
        "#     'O': 0,\n",
        "#     'B-product': 1, 'I-product': 2,\n",
        "#     'B-field': 3, 'I-field': 4,\n",
        "#     'B-task': 5, 'I-task': 6,\n",
        "#     'B-researcher': 7, 'I-researcher': 8,\n",
        "#     'B-country': 9, 'I-country': 10,\n",
        "#     'B-politician': 11, 'I-politician': 12,\n",
        "#     'B-election': 13, 'I-election': 14,\n",
        "#     'B-person': 15, 'I-person': 16,\n",
        "#     'B-organisation': 17, 'I-organisation': 18,\n",
        "#     'B-location': 19, 'I-location': 20,\n",
        "#     'B-misc': 21, 'I-misc': 22,\n",
        "#     'B-politicalparty': 23, 'I-politicalparty': 24,\n",
        "#     'B-event': 25, 'I-event': 26,\n",
        "#     'B-scientist': 27, 'I-scientist': 28,\n",
        "#     'B-university': 29, 'I-university': 30,\n",
        "#     'B-discipline': 31, 'I-discipline': 32,\n",
        "#     'B-enzyme': 33, 'I-enzyme': 34,\n",
        "#     'B-protein': 35, 'I-protein': 36,\n",
        "#     'B-chemicalelement': 37, 'I-chemicalelement': 38,\n",
        "#     'B-chemicalcompound': 39, 'I-chemicalcompound': 40,\n",
        "#     'B-astronomicalobject': 41, 'I-astronomicalobject': 42,\n",
        "#     'B-academicjournal': 43, 'I-academicjournal': 44,\n",
        "#     'B-theory': 45, 'I-theory': 46,\n",
        "#     'B-award': 47, 'I-award': 48,\n",
        "#     'B-musicgenre': 49, 'I-musicgenre': 50,\n",
        "#     'B-song': 51, 'I-song': 52,\n",
        "#     'B-band': 53, 'I-band': 54,\n",
        "#     'B-album': 55, 'I-album': 56,\n",
        "#     'B-musicalartist': 57, 'I-musicalartist': 58,\n",
        "#     'B-musicalinstrument': 59, 'I-musicalinstrument': 60,\n",
        "#     'B-book': 61, 'I-book': 62,\n",
        "#     'B-writer': 63, 'I-writer': 64,\n",
        "#     'B-poem': 65, 'I-poem': 66,\n",
        "#     'B-magazine': 67, 'I-magazine': 68,\n",
        "#     'B-literarygenre': 69, 'I-literarygenre': 70,\n",
        "#     'B-programlang': 71, 'I-programlang': 72,\n",
        "#     'B-algorithm': 73, 'I-algorithm': 74,\n",
        "#     'B-metrics': 75, 'I-metrics': 76,\n",
        "#     'B-conference': 77, 'I-conference': 78\n",
        "# }\n",
        "\n",
        "label_map = {\n",
        "    'O': 0,\n",
        "    'B-country': 9, 'I-country': 10,\n",
        "    'B-politician': 11, 'I-politician': 12,\n",
        "    'B-election': 13, 'I-election': 14,\n",
        "    'B-person': 15, 'I-person': 16,\n",
        "    'B-organisation': 17, 'I-organisation': 18,\n",
        "    'B-location': 19, 'I-location': 20,\n",
        "    'B-misc': 21, 'I-misc': 22,\n",
        "    'B-politicalparty': 23, 'I-politicalparty': 24,\n",
        "    'B-event': 25, 'I-event': 26\n",
        "}\n"
      ],
      "metadata": {
        "id": "OoTQQa0SO_jm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration - use GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "ZS7Jcgp5j4HX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "model = BertForTokenClassification.from_pretrained('/content/drive/MyDrive/Capstone Project Data/Direct Fine-Tuning')\n",
        "tokenizer = BertTokenizerFast.from_pretrained('/content/drive/MyDrive/Capstone Project Data/Direct Fine-Tuning')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "UUfCrCyiWMaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader setup\n",
        "dev_dataset = AIDataset('/content/drive/MyDrive/Capstone Project Data/English NER data (Domains)/politics/test.txt', tokenizer, label_map)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "lOL_jiJmWMVW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of labels in the model:\", model.num_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ol9zQBmyaFKs",
        "outputId": "cc244a30-5975-4c50-f294-9f1d914e5bf7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of labels in the model: 79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "\n",
        "all_predictions = []\n",
        "all_true_labels = []\n",
        "\n",
        "for batch in dev_loader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels_tensor = batch['labels'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n",
        "    labels_tensor = labels_tensor.detach().cpu().numpy()\n",
        "\n",
        "    # Correctly handling predictions and true labels to ensure consistent lengths\n",
        "    for i in range(input_ids.size(0)):\n",
        "        input_length = int(attention_mask[i].sum())\n",
        "        pred = predictions[i][:input_length]\n",
        "        true = labels_tensor[i][:input_length]\n",
        "\n",
        "        # Filter out '-100' values used for subword tokens in labels\n",
        "        valid_indices = [idx for idx, label in enumerate(true) if label != -100]\n",
        "\n",
        "        # Append valid predictions and labels\n",
        "        all_predictions.extend(pred[valid_indices])\n",
        "        all_true_labels.extend(true[valid_indices])\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "unique_labels = sorted(set(all_predictions) | set(all_true_labels))\n",
        "target_names = [k for k, v in sorted(label_map.items(), key=lambda item: item[1]) if v in unique_labels]\n",
        "\n",
        "report = classification_report(\n",
        "    all_true_labels,\n",
        "    all_predictions,\n",
        "    labels=unique_labels,\n",
        "    target_names=target_names,\n",
        "    zero_division=0\n",
        ")\n",
        "\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "id": "s6TENEUCWMSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Fey3rcN3D2E"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_UkkBhvt3DIq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OhHkQIzxWMP_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JRDMQPIFWMNJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vg-BGPZQWMC2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "83Qj4_F1WY7Z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nI3xhB7IWY3m"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cwmAm2FGWY0r"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}