{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "CtTfwwaOZqCk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "id": "N5055cQnbVNz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast, BertForTokenClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "HtF4mmBGyytq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEnidL01taI8",
        "outputId": "91c9009a-cf0c-489f-a2e6-bbb1662f6256"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import f1_score, classification_report\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, texts, tags, tokenizer, max_len, label_map):\n",
        "        self.texts = texts\n",
        "        self.tags = tags\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.label_map = label_map\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = self.texts[item]\n",
        "        tags = self.tags[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        labels = [self.label_map[tag] if tag in self.label_map else self.label_map['O'] for tag in tags]\n",
        "        labels = [-100] + labels[:self.max_len-2] + [-100] * (self.max_len - len(labels) - 1)\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(labels, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Define label_map\n",
        "label_map = {\n",
        "    'O': 0,\n",
        "    'B-product': 1, 'I-product': 2,\n",
        "    'B-field': 3, 'I-field': 4,\n",
        "    'B-task': 5, 'I-task': 6,\n",
        "    'B-researcher': 7, 'I-researcher': 8,\n",
        "    'B-country': 9, 'I-country': 10,\n",
        "    'B-politician': 11, 'I-politician': 12,\n",
        "    'B-election': 13, 'I-election': 14,\n",
        "    'B-person': 15, 'I-person': 16,\n",
        "    'B-organisation': 17, 'I-organisation': 18,\n",
        "    'B-location': 19, 'I-location': 20,\n",
        "    'B-misc': 21, 'I-misc': 22,\n",
        "    'B-politicalparty': 23, 'I-politicalparty': 24,\n",
        "    'B-event': 25, 'I-event': 26,\n",
        "    'B-scientist': 27, 'I-scientist': 28,\n",
        "    'B-university': 29, 'I-university': 30,\n",
        "    'B-discipline': 31, 'I-discipline': 32,\n",
        "    'B-enzyme': 33, 'I-enzyme': 34,\n",
        "    'B-protein': 35, 'I-protein': 36,\n",
        "    'B-chemicalelement': 37, 'I-chemicalelement': 38,\n",
        "    'B-chemicalcompound': 39, 'I-chemicalcompound': 40,\n",
        "    'B-astronomicalobject': 41, 'I-astronomicalobject': 42,\n",
        "    'B-academicjournal': 43, 'I-academicjournal': 44,\n",
        "    'B-theory': 45, 'I-theory': 46,\n",
        "    'B-award': 47, 'I-award': 48,\n",
        "    'B-musicgenre': 49, 'I-musicgenre': 50,\n",
        "    'B-song': 51, 'I-song': 52,\n",
        "    'B-band': 53, 'I-band': 54,\n",
        "    'B-album': 55, 'I-album': 56,\n",
        "    'B-musicalartist': 57, 'I-musicalartist': 58,\n",
        "    'B-musicalinstrument': 59, 'I-musicalinstrument': 60,\n",
        "    'B-book': 61, 'I-book': 62,\n",
        "    'B-writer': 63, 'I-writer': 64,\n",
        "    'B-poem': 65, 'I-poem': 66,\n",
        "    'B-magazine': 67, 'I-magazine': 68,\n",
        "    'B-literarygenre': 69, 'I-literarygenre': 70,\n",
        "    'B-programlang': 71, 'I-programlang': 72,\n",
        "    'B-algorithm': 73, 'I-algorithm': 74,\n",
        "    'B-metrics': 75, 'I-metrics': 76,\n",
        "    'B-conference': 77, 'I-conference': 78\n",
        "}\n",
        "\n",
        "\n",
        "# Load model and tokenizer with the correct number of labels\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(label_map))\n",
        "\n",
        "# Example function to read NER data\n",
        "def read_ner_data(file_path):\n",
        "    texts, tags = [], []\n",
        "    with open(file_path, 'r') as file:\n",
        "        words, labels = [], []\n",
        "        for line in file:\n",
        "            if line.startswith('-DOCSTART-') or line == '\\n':\n",
        "                if words:\n",
        "                    texts.append(' '.join(words))\n",
        "                    tags.append(labels)\n",
        "                    words, labels = [], []\n",
        "                continue\n",
        "            splits = line.strip().split()\n",
        "            words.append(splits[0])\n",
        "            labels.append(splits[-1])\n",
        "        if words:\n",
        "            texts.append(' '.join(words))\n",
        "            tags.append(labels)\n",
        "    return texts, tags\n",
        "\n",
        "# Load data\n",
        "file_path = '/content/drive/MyDrive/Capstone Project Data/English NER data (Domains)/science/train.txt'\n",
        "texts, tags = read_ner_data(file_path)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "max_len = 128\n",
        "dataset = NERDataset(texts, tags, tokenizer, max_len, label_map)\n",
        "loader = DataLoader(dataset, batch_size=32)\n",
        "\n",
        "idx_to_tag = {idx: tag for tag, idx in label_map.items()}\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "model.eval()\n",
        "model.to('cuda')\n",
        "predictions, true_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in loader:\n",
        "        input_ids = batch['input_ids'].to('cuda')\n",
        "        attention_mask = batch['attention_mask'].to('cuda')\n",
        "        labels = batch['labels'].to('cuda')\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        pred_labels = torch.argmax(logits, dim=2)\n",
        "\n",
        "        # Iterate over each sequence in the batch\n",
        "        for i in range(input_ids.shape[0]):\n",
        "            input_length = torch.sum(attention_mask[i]).item()  # Actual length of the sequence\n",
        "            # Extract the predictions and true labels using the actual length\n",
        "            # Excluding CLS and SEP for predictions and true labels which are ignored by -100\n",
        "            pred_slice = pred_labels[i][1:input_length-1].tolist()\n",
        "            true_slice = labels[i][1:input_length-1].tolist()\n",
        "            # Convert indices to tags, excluding -100\n",
        "            predictions.append([idx_to_tag[p] for p, t in zip(pred_slice, true_slice) if t != -100])\n",
        "            true_labels.append([idx_to_tag[t] for t in true_slice if t != -100])\n",
        "\n",
        "# Calculate F1 Score\n",
        "print(\"F1 Score:\", f1_score(true_labels, predictions))\n",
        "print(classification_report(true_labels, predictions))\n",
        "\n"
      ],
      "metadata": {
        "id": "9sb8viJ5a_tV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}